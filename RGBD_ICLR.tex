\documentclass{article} % For LaTeX2e
\usepackage{iclr2015,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}

\title{Occlusion Detection using Deep Learning}


\author{
author1 \\
%Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
%about author (webpage, alternative address)---\emph{not} for acknowledging
%funding agencies.  Funding acknowledgements go at the end of the paper.} \\
%Department of Computer Science\\
%Cranberry-Lemon University\\
%Pittsburgh, PA 15213, USA \\
%\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
%\And
%Ji Q. Ren \& Yevgeny LeNet \\
%Department of Computational Neuroscience \\
%University of the Witwatersrand \\
%Joburg, South Africa \\
%\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version

%\iclrconference % Uncomment if submitted as conference paper instead of workshop

\begin{document}


\maketitle

\begin{abstract}
Occlusion edges in images which correspond to range discontinuity in the scene from the point of view of the observer are an important prerequisite for many vision and mobile robot tasks. They can be extracted from range data however extracting them from image and videos would be extremely beneficial. We develop an unsupervised deep learning technique to identify occlusion edges in images and videos. Training data for the deep learning is generated from the depth values of an RGBD sensor, a color camera which also gives pixel ranges for short distances (< 5m).
%The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
%right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
%The word \textsc{Abstract} must be centered, in small caps, and in point size 12. Two
%line spaces precede the abstract. The abstract must be limited to one
%paragraph.
\end{abstract}

\section{INtroduction}

%ICLR requires electronic submissions, processed by
%\url{http://arxiv.org}. See ICLR's website for more instructions.
%
%Authors can choose to submit to the workshop track or the conference
%track, and this choice must be mentioned at the top of every
%page of the paper. This can be achieved by inserting the {\tt
%  {\textbackslash}iclrconference} statement in your latex submission
%file (for the conference track) or not (for the workshop track).
%
%If your paper is ultimately accepted, the statement {\tt
%  {\textbackslash}iclrfinalcopy} should be inserted to adjust the
%format to the camera ready requirements.
%
%The format for the submissions is a variant of the NIPS format.
%Please read carefully the instructions below, and follow them
%faithfully.

\subsection{Motivation}
Occlusion edge detection is a fundamental capability of computer vision systems as is evident from the number of applications and significant attention it has received
% various other literature on occlusion edges
\cite{jacobson2012online,ayvaci2011detachable,sargin2009probabilistic,marshall1996occlusion,stein2009occlusion}.
% TODO discuss these more individually
Occlusion edges are useful for a wide array of tasks including object recognition, feature selection, grasping, obstacle avoidance, navigating, path-planning,  localisation, mapping, stereo-vision and optic flow.
In addition to numerous applications the concept of occlusions edges is support by the human visual perception research where it is referred to as figure/ground determination. Common fate motion \cite{wagemans2012century} of occlusions edges and the internal texture is one of the primary mechanisms for the occluding edge determination in humans.

% figure/ground assignment
Once occlusion boundaries have been established depth order of regions become possible \cite{sundberg2011occlusion,smith2004layered} which aids navigation, slam and path planning.


% feature selection
Occlusion edges help image feature selection by rejecting features generated from regions that span an occlusion edge.  Because these are dependent on viewpoint position.  Removing these variant feature saves on further processing and increases recognition accuracy.  Interest point invariance under observer pose is an essential component of feature performance \cite{gil2010comparative}.

% object recognition
Predominately objects are delineated by their spatial boundaries and particularly for rigid objects their shapes are intrinsically invariant as well as often being tied to their function.
This form-function link means form as opposed to appearance is often more invariant which aids recognition, especially of object class rather than  recognising particular object instances.
Object recognition by matching shape of object means it may have any colour or pattern but will still be recognised.  This is not the case for state of art sift based object recognition.
In some situations the shape the object is better for recognition rather than its appearance, which can be easily dramatically altered e.g.\ painted objects, camouflage and people wearing different clothes.

% stereo and optic flow
Knowing the occlusion edges helps with stereo vision and optic flow algorithms
Stereo vision and depth discontinuities.
Stereo vision approaches often ignore information which is absent in either image. In \cite{belhumeur1992bayesian} they instead take advantage of this unilateral information to as a strong clue to depth discontinuities.
Optic flow is often a precursor as in \cite{sundberg2011occlusion} with good motion edge determination accomplished by bi-directional frame differencing for sequential video frames.  \cite{sundberg2011occlusion} also points out that unlike color and stereopsis all visual species use motion as a cue, implying its importance.

\cite{klappstein2009moving}

% grasping and manipulation
The geometric edges of objects demarcate their spatial extent helping with grasping, manipulation as well as maneuvering through the world without collision.
Knowledge of the occlusion edges is essential for effective moving rigid and articulated object tracking.  By selecting regions of the object that do not span occlusion edges they are more likely to be completely on the object and hence more reliably tracked through methods such as template matching.

%\subsection{Style}
%
%Papers to be submitted to ICLR 2015 must be prepared according to the
%instructions presented here.
%
%Please note that we have introduced automatic line number generation
%into the style file for \LaTeXe. This is to help reviewers
%refer to specific lines of the paper when they make their comments. Please do
%NOT refer to these line numbers in your paper as they will be removed from the
%style file for the final version of accepted papers.
%
%Authors are required to use the ICLR \LaTeX{} style files obtainable at the
%ICLR website. Please make sure you use the current files and
%not previous versions. Tweaking the style files may be grounds for rejection.
%
%\subsection{Retrieval of style files}
%
%The style files for ICLR and other conference information are available on the World Wide Web at
%\begin{center}
%   \url{http://www.iclr.cc/}
%\end{center}
%The file \verb+iclr2015.pdf+ contains these
%instructions and illustrates the
%various formatting requirements your ICLR paper must satisfy.
%Submissions must be made using \LaTeX{} and the style files
%\verb+iclr2015.sty+ and \verb+iclr2015.bst+ (to be used with \LaTeX{}2e). The file
%\verb+iclr2015.tex+ may be used as a ``shell'' for writing your paper. All you
%have to do is replace the author, title, abstract, and text of the paper with
%your own.
%
%The formatting instructions contained in these style files are summarized in
%sections \ref{gen_inst}, \ref{headings}, and \ref{others} below.

%\section{General formatting instructions}
%\label{gen_inst}
%
%The text must be confined within a rectangle 5.5~inches (33~picas) wide and
%9~inches (54~picas) long. The left margin is 1.5~inch (9~picas).
%Use 10~point type with a vertical spacing of 11~points. Times New Roman is the
%preferred typeface throughout. Paragraphs are separated by 1/2~line space,
%with no indentation.
%
%Paper title is 17~point, in small caps and left-aligned.
%All pages should start at 1~inch (6~picas) from the top of the page.
%
%Authors' names are
%set in boldface, and each name is placed above its corresponding
%address. The lead author's name is to be listed first, and
%the co-authors' names are set to follow. Authors sharing the
%same address can be on the same line.
%
%Please pay special attention to the instructions in section \ref{others}
%regarding figures, tables, acknowledgments, and references.
%
%\section{Headings: first level}
%\label{headings}
%
%First level headings are in small caps,
%flush left and in point size 12. One line space before the first level
%heading and 1/2~line space after the first level heading.
%
%\subsection{Headings: second level}
%
%Second level headings are in small caps,
%flush left and in point size 10. One line space before the second level
%heading and 1/2~line space after the second level heading.
%
%\subsubsection{Headings: third level}
%
%Third level headings are in small caps,
%flush left and in point size 10. One line space before the third level
%heading and 1/2~line space after the third level heading.
%
%\section{Citations, figures, tables, references}
%\label{others}
%
%These instructions apply to everyone, regardless of the formatter being used.
%
%\subsection{Citations within the text}
%
%Citations within the text should be based on the {\tt natbib} package
%and include the authors' last names and year (with the ``et~al.'' construct
%for more than two authors). When the authors or the publication are
%included in the sentence, the citation should not be in parenthesis (as
%in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
%should be in parenthesis (as in ``Deep learning shows promise to make progress towards AI~\citep{Bengio+chapter2007}.'').
%
%The corresponding references are to be listed in alphabetical order of
%authors, in the \textsc{References} section. As to the format of the
%references themselves, any style is acceptable as long as it is used
%consistently.

\section{Background Material}
% Assumptions
Throughout this work it is assumed that appearance edges are a necessary but not sufficient condition for occlusion edges.
This assumption is rarely violated in real world environments but when it is then even the human visual system fails.
% see those pictures of painted people standing against a similar painted wall

% edges and not planes for mapping
There is utility of geometric edges for localisation and mapping (SLAM) for mobile robots.  Many textureless environments are not suitable for feature based SLAM techniques despite being relatively common in indoor environments.  However, maps based on occlusion and geometric edges will still allow localisation even in these low texture regions.
For indoor mapping, planes seem a natural consideration for landmarks, and indeed numerous researchers have explored plane based mapping \cite{pathak2010}. % TODO more citations?
Although less common in robotic experiments, there are places not suitable for planar mapping including buildings with curved walls, natural outdoor environments and extremely cluttered scenes such as those found in search and rescue scenarios.  Although planes can be a good way of compressing map information, an observed planar surface is not as constraining to robot pose as feature points and edges.

Another strong motivation for using the geometric edges is that they allow localisation by both range and image sensors.
For many environments, considering only geometric edges, removes floors, walls and ceilings leaving the elements that lie along the intersection of planes or in cluttered regions, resulting in significant compression of the map data.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Figures/edgevoxels_blend_image.jpg}
    %\includegraphics[width=1\linewidth]{mason2011_hallway_map}
    %\includegraphics[width=1\linewidth]{mason2011_hallway_edgemap}
    \caption{A voxel map and the corresponding geometric edges for the \textit{mason hallway} dataset.}
    \label{fig_voxel_edge_map_example}
\end{figure}


% TODO add another image with only occlusion edges
\begin{figure*}[t]
	\centering
    \includegraphics[width=0.32\linewidth]{Figures/cylinder.png}
    \includegraphics[width=0.32\linewidth]{Figures/cylinder_image_edges.png}
    \includegraphics[width=0.32\linewidth]{Figures/cylinder_range_edges.png}
    \caption{Image with associated edges due to appearance and due to geometry.}
    \label{fig_edge_types}
\end{figure*}

% definition of edge pixels
A rigorous definition of edge pixels is difficult.  Edges manifest along paths of high contrast in images, and are due to four main reasons.
% it is the abrupt transition of one material to another either because 4 types of appearance edge
\begin{enumerate}
\item Texture change --- Abrupt change in surface color.
\item Lighting change --- Sharp shadows.
\item Range discontinuity --- Abrupt change in distance from the observer.
\item Surface normal change --- E.g. intersection of two planes.
\end{enumerate}

% edge distinction different type of edges
It is important to appreciate the distinction in the causes of image edges.   Texture change and illumination edges are not observed by 3D sensors. So the remaining geometric edge types are range discontinuities and abrupt surface normal changes.  Surface normal changes are pose invariant, however edges due to range discontinuities can vary with observer position.
These surface normal and range discontinuities are illustrated in the last image of Fig.~\ref{fig_edge_types}.  The cylinder sides in Fig.~\ref{fig_edge_types} are examples of range discontinuities.  The position of these edges varies in 3D space as the position of the observer shifts whereas the cylinder rim edge position is consistent regardless of observer position.

For use in mapping, we desire the following characteristics from extracted edge voxels: they should be generally invariant to rotation and translation, and they should be helpful in terms of constraining pose.   We hence directly seek the third and fourth type of edges.

\section{Related work}
% possibly related literature and motivation called primal sketch.

There are many motivations for occlusion edge determination, more than can be satisfactorily listed here, good reasons are included in numerous papers \cite{hoiem2007recovering, bobick1999large, feldman2007motion, feldman2008motion, humayun2011learning}.

% why edges and not just features
At this juncture a distinction should be made between maps containing the positions of landmarks or features and denser maps.  Feature maps aid localisation but do not allow obstacle detection.
Dense maps consisting of point clouds or occupancy grids enable localisation, path planning and obstacle avoidance.

% density of different map representations
The following list summarises these maps in descending sparsity and places the mapping of edges into the context of the broader research.  Sparser maps are smaller, easier to store and quicker to process however they do not work with as wide a range of robot tasks.
\begin{itemize}
\item 3D occupancy grid --- Extension of 2D occupancy grids
\item Occupied voxel list --- Occupied voxels only
\item Geometric edge map --- Occlusion and/or surface normal edges
\item Feature map --- List of point features and their covariances
\end{itemize}
% paper from drummond
In \cite{eade2006} they extend conventional landmark based SLAM to incorporate edge information by the extraction of edgelets from the scene image.

% Acceleration of MROL/ICP or any SLAM with edge extraction
The building of maps whilst considering all occupied voxels has proved successful for both indoor and outdoor environments \cite{ryde2011a}.  There is a continuum in sparsity ranging from full 3D occupancy to feature maps.  Feature extraction, whilst extremely helpful, comes at a price, namely reduced generalisation where mapping will fail in environments without the requisite features.  It is observed that for indoor environments, while reliable point features can sometimes be absent, there are usually edge features. Edge mapping is faster and the associated maps are smaller and therefore require less memory.
In the worst case, if there are not enough edges available it is possible to resort to full matching of the occupied voxels.  For environments with planar surfaces there are far fewer edge voxels than occupied voxels.
For conventional appearance edge extraction the structure tensor has been widely applied in image \cite{harris1988} and video analysis \cite{kuhne2001tensor, wang2003automatic}.

The benefits of geometric edges for simultaneous localization and mapping (SLAM) has been established in \cite{ryde2012extracting}.
Other approaches for detect geometric edges in 3D data are a keypoint detector based on a 3D extension of the Harris corner operator in the Point Cloud Library \cite{rusu2011_icra_PCL}. This detector operates on local normals of points
A related approach for selecting interest points on 3D meshes was introduced in \cite{sipiran2011harris}.

\section{Generating the Training data}
To create an unsupervised training procedure we train with the depth data from an RGB-D sensor.


\begin{figure*}
    \centering
    \includegraphics[width=0.32\linewidth]{Figures/RGB-00000.png}
    \includegraphics[width=0.32\linewidth]{Figures/Z-00000.png}
    \includegraphics[width=0.32\linewidth]{Figures/edges-00000.png}
    \caption{Example RGB, depth and classification frames from the training
        data generation procedure.  In the classification frame gray signifies
        no edge, occlusion edges are white and black is for no or unreliable
        data. }
    \label{fig_rgb_edge}
\end{figure*}

\begin{verbatim}
D = xyz_rgbs[:, :, 2]
RGB = xyz_rgbs[:, :, 3:].astype(np.uint8)
D[D > max_range] = 0

# simple edge detect on D to provide classification
E = ndimage.gaussian_gradient_magnitude(D, edge_sigma)
# filter out bad measurements e.g. too far or absorbing surface
C = np.zeros(E.shape)
C[E > edge_strength_threshold] = 1
structure = np.ones((dilate_size, dilate_size), dtype=np.bool)
bad_inds = ndimage.binary_dilation(D==0, structure=structure, iterations=1)
C[bad_inds] = -1
\end{verbatim}

\section{Results}

\begin{figure}
    \vspace{2in}
    \caption{Plot of occlusion edge recognition accuracy as a function of patch size}
\end{figure}

%\subsection{Footnotes}
%
%Indicate footnotes with a number\footnote{Sample of the first footnote} in the
%text. Place the footnotes at the bottom of the page on which they appear.
%Precede the footnote with a horizontal rule of 2~inches
%(12~picas).\footnote{Sample of the second footnote}
%
%\subsection{Figures}
%
%All artwork must be neat, clean, and legible. Lines should be dark
%enough for purposes of reproduction; art work should not be
%hand-drawn. The figure number and caption always appear after the
%figure. Place one line space before the figure caption, and one line
%space after the figure. The figure caption is lower case (except for
%first word and proper nouns); figures are numbered consecutively.
%
%Make sure the figure caption does not get separated from the figure.
%Leave sufficient space to avoid splitting the figure and figure caption.
%
%You may use color figures.
%However, it is best for the
%figure captions and the paper body to make sense if the paper is printed
%either in black/white or in color.
%\begin{figure}[h]
%\begin{center}
%%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%\end{center}
%\caption{Sample figure caption.}
%\end{figure}
%
%\subsection{Tables}
%
%All tables must be centered, neat, clean and legible. Do not use hand-drawn
%tables. The table number and title always appear before the table. See
%Table~\ref{sample-table}.
%
%Place one line space before the table title, one line space after the table
%title, and one line space after the table. The table title must be lower case
%(except for first word and proper nouns); tables are numbered consecutively.
%
%\begin{table}[t]
%\caption{Sample table title}
%\label{sample-table}
%\begin{center}
%\begin{tabular}{ll}
%\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
%\\ \hline \\
%Dendrite         &Input terminal \\
%Axon             &Output terminal \\
%Soma             &Cell body (contains cell nucleus) \\
%\end{tabular}
%\end{center}
%\end{table}
%
%\section{Final instructions}
%Do not change any aspects of the formatting parameters in the style files.
%In particular, do not modify the width or length of the rectangle the text
%should fit into, and do not change font sizes (except perhaps in the
%\textsc{References} section; see below). Please note that pages should be
%numbered.
%
%\section{Preparing PostScript or PDF files}
%
%Please prepare PostScript or PDF files with paper size ``US Letter'', and
%not, for example, ``A4''. The -t
%letter option on dvips will produce US Letter files.
%
%Consider directly generating PDF files using \verb+pdflatex+
%(especially if you are a MiKTeX user).
%PDF figures must be substituted for EPS figures, however.
%
%Otherwise, please generate your PostScript and PDF files with the following commands:
%\begin{verbatim}
%dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
%ps2pdf mypaper.ps mypaper.pdf
%\end{verbatim}
%
%\subsection{Margins in LaTeX}
%
%Most of the margin problems come from figures positioned by hand using
%\verb+\special+ or other commands. We suggest using the command
%\verb+\includegraphics+
%from the graphicx package. Always specify the figure width as a multiple of
%the line width as in the example below using .eps graphics
%\begin{verbatim}
%   \usepackage[dvips]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]{myfile.eps}
%\end{verbatim}
%or % Apr 2009 addition
%\begin{verbatim}
%   \usepackage[pdftex]{graphicx} ...
%   \includegraphics[width=0.8\linewidth]{myfile.pdf}
%\end{verbatim}
%for .pdf graphics.
%See section 4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})
%
%A number of width problems arise when LaTeX cannot properly hyphenate a
%line. Please give LaTeX hyphenation hints using the \verb+\-+ command.


\subsubsection*{Acknowledgments}

%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.

\bibliography{RGBD_DL}
\bibliographystyle{iclr2015}

\end{document}
